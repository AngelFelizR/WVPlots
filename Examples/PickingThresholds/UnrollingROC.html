<h1 id="unrolling-the-roc">Unrolling the ROC</h1>
<p>Nina Zumel</p>
<p>In our data science teaching, we present the ROC plot (and the area under the curve of the plot, or AUC) as a useful tool for evaluating scoring classifier models, and for comparing multiple such models. The ROC is informative and useful, but itâ€™s also perhaps overly concise for a beginner. This leads to a lot of questions from the students: what does the ROC tell us about a model? Why is a bigger AUC better? <a href="https://win-vector.com/2013/01/17/more-on-rocauc/">What does it all <em>mean</em></a>?</p>
<p>We'll start with a simple synthetic example, where <code>rscore</code> is the score produced by a trained model, and <code>y</code> is the actual outcome in the evaluation data set (<code>TRUE</code> or <code>FALSE</code>). We'll assume for this discussion that the model score is a number in the unit interval, as would be true for a probability model.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1452225</span>)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co"># sigmoid maps the real line to the unit interval</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">sigmoid =<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">  <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x))</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co"># data with two different regimes of behavior</span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">d &lt;-<span class="st"> </span><span class="kw">rbind</span>(</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">  <span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">    <span class="dt">score =</span> <span class="kw">sigmoid</span>(<span class="fl">0.4</span><span class="op">*</span>(<span class="kw">rnorm</span>(<span class="dv">1000</span>) <span class="op">-</span><span class="st"> </span><span class="fl">2.5</span>) ),</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">    <span class="dt">y =</span> <span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>), <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.02</span>, <span class="fl">0.98</span>), </a>
<a class="sourceLine" id="cb1-13" data-line-number="13">               <span class="dt">size =</span> <span class="dv">1000</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)),</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">  <span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">    <span class="dt">score =</span> <span class="kw">sigmoid</span>( <span class="fl">0.4</span><span class="op">*</span>(<span class="kw">rnorm</span>(<span class="dv">200</span>) <span class="op">+</span><span class="st"> </span><span class="fl">2.5</span>) ), </a>
<a class="sourceLine" id="cb1-16" data-line-number="16">    <span class="dt">y =</span> <span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>), <span class="dt">size =</span> <span class="dv">200</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb1-17" data-line-number="17">)</a></code></pre></div>
<p>Here's the ROC, and its associated AUC:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="kw">library</span>(WVPlots)</a></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">ROCPlot</span>(d, <span class="st">&quot;score&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="dt">truthTarget=</span><span class="ot">TRUE</span>, <span class="dt">title=</span><span class="st">&quot;Model ROC&quot;</span>) </a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-5-1.png" /><!-- --></p>
<p>Every point of the ROC corresponds to a given <em>threshold</em>; datums that score at least the threshold value are classified as positive (or TRUE, in this case), and datums that score below the threshold are classified as negative, or FALSE. <strong>So the ROC represents the false positive and true positive rates of <em>all possible</em> <a href="https://win-vector.com/2020/08/07/dont-use-classification-rules-for-classification-problems/">classification rules</a> that can be defined from this model by varying the threshold</strong>. I've marked the point that corresponds to the threshold 0.5, which is the most commonly used threshold.</p>
<h2 id="the-roc-unrolled">The ROC Unrolled</h2>
<p>To make the information in the ROC more explicit, let's plot the true positive rates and false positive rates off all possible classification rules as a function of threshold. We can do this with the <code>ThresholdPlot()</code> function from the <code>WVPlots</code> package.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">ThresholdPlot</span>(d, <span class="st">&quot;score&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="dt">title=</span><span class="st">&quot;ROC &#39;unrolled&#39;&quot;</span>,</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">              <span class="dt">truth_target =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">              <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;true_positive_rate&quot;</span>, <span class="st">&quot;false_positive_rate&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.5</span>, <span class="dt">color =</span> <span class="st">&quot;#d95f02&quot;</span>)</a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-6-1.png" /><!-- --></p>
<p>I've added a horizontal line at <code>threshold = 0.5</code>.</p>
<p>The threshold plot and the ROC give us the exact same tradeoff information, but the threshold plot makes the relationship of performance to threshold value explicit This can be useful if you are willing to tweak the threshold to make a tradeoff. For example, if avoiding false positives is more important to you than catching all the positive instances, you could try a stricter threshold like 0.75. In this case, the classifier would make fewer false positive errors, but it would also detect far fewer positive examples, as well. On the other hand, the ROC (and the AUC) are more convenient for comparing multiple scoring models.</p>
<h3 id="some-strawman-models-for-intuition">Some Strawman Models for Intuition</h3>
<p>The threshold plot can also give us further intuition about the ROC. We can see that as the threshold goes to zero, both the true and false positive rates go to one (the upper right hand corner of the ROC). As the threshold goes to one, both the rates go to zero (the lower left hand corner of the ROC). As the threshold increases from zero to one, it traces out the rest of the ROC from upper right to bottom left, representing all the tradeoffs between true positive rate and false positive rate that are possible with a given model.</p>
<p><strong>The Ideal Model</strong></p>
<p>The ideal model would predict every instance perfectly; positives would score one, negatives would score zero. Let's see what that would look like.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">ideal &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">score =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">50</span>), <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">50</span>)),</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">                    <span class="dt">y =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="ot">TRUE</span>, <span class="dv">50</span>), <span class="kw">rep</span>(<span class="ot">FALSE</span>, <span class="dv">50</span>)) )</a>
<a class="sourceLine" id="cb5-3" data-line-number="3"></a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="kw">ThresholdPlot</span>(ideal, <span class="st">&quot;score&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="dt">title=</span><span class="st">&quot;Ideal model&quot;</span>,</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">              <span class="dt">truth_target =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">              <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;true_positive_rate&quot;</span>, <span class="st">&quot;false_positive_rate&quot;</span>)) </a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-7-1.png" /><!-- --></p>
<p>(The endpoints are convention). With an ideal model, any threshold (except exactly zero or one) produces a classification rule with a true positive rate of one and and a false positive rate of zero. This traces out an ROC that's exactly the unit square:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">ROCPlot</span>(ideal, <span class="st">&quot;score&quot;</span>, <span class="st">&quot;y&quot;</span>, </a>
<a class="sourceLine" id="cb6-2" data-line-number="2">        <span class="dt">truthTarget =</span> <span class="ot">TRUE</span>, <span class="dt">title =</span> <span class="st">&quot;Ideal model&quot;</span>)</a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-8-1.png" /><!-- --></p>
<p>So now we know <strong><em>the ideal model has an AUC of 1</em></strong>. We want our actual model to trace out a curve as close to the ideal model as possible; such a model should have an AUC close to 1.</p>
<p><strong>The Random Model</strong></p>
<p>What about a model that doesn't predict anything at all, but just returns random answers?</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">random &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">score =</span> <span class="kw">runif</span>(<span class="dv">1000</span>),</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">                    <span class="dt">y =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="ot">TRUE</span>, <span class="dv">500</span>), <span class="kw">rep</span>(<span class="ot">FALSE</span>, <span class="dv">500</span>)) )</a>
<a class="sourceLine" id="cb7-3" data-line-number="3"></a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="kw">ThresholdPlot</span>(random, <span class="st">&quot;score&quot;</span>, <span class="st">&quot;y&quot;</span>, <span class="dt">title=</span><span class="st">&quot;Random model&quot;</span>,</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">              <span class="dt">truth_target =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">              <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&quot;true_positive_rate&quot;</span>, <span class="st">&quot;false_positive_rate&quot;</span>)) </a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-9-1.png" /><!-- --></p>
<p>Both the true and false positive rates are proportional to the threshold, and the ROC will be close to the line <code>x = y</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">ROCPlot</span>(random, <span class="st">&quot;score&quot;</span>, <span class="st">&quot;y&quot;</span>,</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">        <span class="dt">truthTarget =</span> <span class="ot">TRUE</span>, <span class="dt">title =</span> <span class="st">&quot;Random model&quot;</span>)</a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-10-1.png" /><!-- --></p>
<p><strong><em>A random model has an AUC near 0.5</em></strong>. And a model that has an ROC that dips below the line <code>x = y</code> (and has an AUC less than 0.5) is actually anticorrelated with the true class labels.</p>
<h2 id="the-roc-and-class-prevalence">The ROC and Class Prevalence</h2>
<p>It's worth noting that the ROC depends on the false positive rate (the rate at which negatives are misclassified as positives) and true positive rate (the rate at which positives are correctly classified), <em>neither of which depends on class prevalence</em>. That means that <strong><em>the ROC is independent of class prevalence</em></strong>. In other words, it can look overly optimistic when the target class is rare.</p>
<p>To demonstrate, let's look at two situations that have been designed to generate the same ROC (for the code to generate this example, see <a href="https://github.com/WinVector/WVPlots/blob/main/Examples/PickingThresholds/UnrollingROC.Rmd">the R markdown for this article</a>). The first situation is a model applied to a balanced-class population; the second is a model applied where the positive class is relatively rare (about 20%).</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># Look at class balances</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="kw">table</span>(balanced<span class="op">$</span>y)</a></code></pre></div>
<pre><code>## 
## FALSE  TRUE 
##  1000  1000</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">table</span>(unbalanced<span class="op">$</span>y)</a></code></pre></div>
<pre><code>## 
## FALSE  TRUE 
##  1600   400</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">ROCPlotPair2</span>(<span class="dt">nm1 =</span> <span class="st">&quot;Balanced classes&quot;</span>, <span class="co"># model 1</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2">             <span class="dt">frame1 =</span> balanced,</a>
<a class="sourceLine" id="cb13-3" data-line-number="3">             <span class="dt">xvar1 =</span> <span class="st">&quot;score&quot;</span>,  <span class="dt">truthVar1 =</span> <span class="st">&quot;y&quot;</span>, <span class="dt">truthTarget1 =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb13-4" data-line-number="4">             <span class="dt">nm2 =</span><span class="st">&quot;Unbalanced classes&quot;</span>, <span class="co"># model 2</span></a>
<a class="sourceLine" id="cb13-5" data-line-number="5">             <span class="dt">frame2 =</span> unbalanced,</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">             <span class="dt">xvar2 =</span> <span class="st">&quot;score&quot;</span>, <span class="dt">truthVar2 =</span> <span class="st">&quot;y&quot;</span>, <span class="dt">truthTarget2 =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb13-7" data-line-number="7">             <span class="dt">title =</span> <span class="st">&quot;Comparing different class prevalences&quot;</span>,</a>
<a class="sourceLine" id="cb13-8" data-line-number="8">             <span class="dt">estimate_sig =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-12-1.png" /><!-- --></p>
<p>As designed, the ROC is nearly identical in both situations, with a high AUC. These both look like good models.</p>
<p>But suppose instead of false positive rate, you are worried about <em>precision</em> (the probability that a instance predicted to be positive is actually positive). With respect to precision, the two models behave quite differently.</p>
<p><img src="UnrollingROC_files/figure-gfm/unnamed-chunk-13-1.png" /><!-- --></p>
<p>Both models have identical recall (which is the same as true positive rate, and is independent of class prevalence), but the model that is applied to a population where the positive class is rare will have much lower precision for any useful setting of the classifier threshold.</p>
<p>Incidentally, this is why when evaluating what a positive result of a medical screening test means, it's important to know the prevalence of the disease (or other medical condition) in the population. Medical screening tests are generally designed to meet certain levels of sensitivity (true positive rate) and specificity (1 - false positive rate); these measures are independent of the disease prevalence.</p>
<p>Suppose that the example in the graph above showed a screening test applied to two different populations. We want to make sure that we identify everyone who has the disease, so we set the threshold at 0.4 (the vertical line on the graph), so the recall is close to one. If 50% of the population has the disease, then 8 out of 10 people who test positive will actually be positive. But if only 20% of the population has the disease, then nearly half the people who test positive are actually negative.</p>
<p>These sort of details are easier to work through with tools that expose tradeoff management, like <code>ThresholdPlot</code>, rather than only examining the ROC.</p>
<h2 id="lets-sum-it-all-up">Let's Sum It All Up</h2>
<p>What have we established in this article?</p>
<ul class="incremental">
<li><p>The ROC and AUC are useful for quickly comparing multiple models. In a comparison, higher AUCs generally mean better models.</p></li>
<li><p>Something like <code>WVPlots::ThresholdPlot</code> can be useful for setting appropriate classifier thresholds. For an extended example of <code>ThresholdPlot</code>, see <a href="https://github.com/WinVector/WVPlots/blob/main/Examples/PickingThresholds/PickingThresholds.md">here</a>.</p></li>
<li><p>An AUC close to one is good; an AUC close to 0.5 means the model doesn't perform much better than random.</p></li>
<li><p>True and false positive rates, and therefore the ROC, are independent of class prevalence. Precision is dependent on class prevalence.</p></li>
<li><p>If your preferred performance metric is dependent on class prevalence, like precision or accuracy, then the ROC may be optimistic when the positive class is rare. <code>ThresholdPlot</code> can also be useful for evaluating a model in this situation.</p></li>
</ul>
<p>Hopefully, you now have a better intuition about the ROC, and what it means.</p>
